{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling of Traffic Stops\n",
    "> Jericho Timbol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Overview\n",
    "For this problem we’ll use data released by the Stanford Open Policing Project (SOPP) for the state of North Carolina, available here https://stacks.stanford.edu/file/druid:py883nd2578/NC-clean.csv.gz. It contains records of 9.6 million police stops in the state between 2000 and 2015. Specifically we will be using this data to explore disparities in traffic stop metrics in regards to demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Modules\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "['NC-2000-000001' 'NC-2000-000002' 'NC-2000-000003' ... 'NC-2001-435771'\n",
      " 'NC-2001-435772' 'NC-2001-435773']\n",
      "state\n",
      "['NC']\n",
      "stop_date\n",
      "['2000-01-01' '2000-01-02' '2000-01-03' '2000-01-04' '2000-01-05'\n",
      " '2000-01-06' '2000-01-07' '2000-01-08' '2000-01-09' '2000-01-10'\n",
      " '2000-01-11' '2000-01-12' '2000-01-13' '2000-01-14' '2000-01-15'\n",
      " '2000-01-16' '2000-01-17' '2000-01-18' '2000-01-19' '2000-01-20'\n",
      " '2000-01-21' '2000-01-22' '2000-01-23' '2000-01-24' '2000-01-25'\n",
      " '2000-01-26' '2000-01-27' '2000-01-28' '2000-01-29' '2000-01-30'\n",
      " '2000-01-31' '2000-02-01' '2000-02-02' '2000-02-03' '2000-02-04'\n",
      " '2000-02-05' '2000-02-06' '2000-02-07' '2000-02-08' '2000-02-09'\n",
      " '2000-02-10' '2000-02-11' '2000-02-12' '2000-02-13' '2000-02-14'\n",
      " '2000-02-15' '2000-02-16' '2000-02-17' '2000-02-18' '2000-02-19'\n",
      " '2000-02-20' '2000-02-21' '2000-02-22' '2000-02-23' '2000-02-24'\n",
      " '2000-02-25' '2000-02-26' '2000-02-27' '2000-02-28' '2000-02-29'\n",
      " '2000-03-01' '2000-03-02' '2000-03-03' '2000-03-04' '2000-03-05'\n",
      " '2000-03-06' '2000-03-07' '2000-03-08' '2000-03-09' '2000-03-10'\n",
      " '2000-03-11' '2000-03-12' '2000-03-13' '2000-03-14' '2000-03-15'\n",
      " '2000-03-16' '2000-03-17' '2000-03-18' '2000-03-19' '2000-03-20'\n",
      " '2000-03-21' '2000-03-22' '2000-03-23' '2000-03-24' '2000-03-25'\n",
      " '2000-03-26' '2000-03-27' '2000-03-28' '2000-03-29' '2000-03-30'\n",
      " '2000-03-31' '2000-04-01' '2000-04-02' '2000-04-03' '2000-04-04'\n",
      " '2000-04-05' '2000-04-06' '2000-04-07' '2000-04-08' '2000-04-09'\n",
      " '2000-04-10' '2000-04-11' '2000-04-12' '2000-04-13' '2000-04-14'\n",
      " '2000-04-15' '2000-04-16' '2000-04-17' '2000-04-18' '2000-04-19'\n",
      " '2000-04-20' '2000-04-21' '2000-04-22' '2000-04-23' '2000-04-24'\n",
      " '2000-04-25' '2000-04-26' '2000-04-27' '2000-04-28' '2000-04-29'\n",
      " '2000-04-30' '2000-05-01' '2000-05-02' '2000-05-03' '2000-05-04'\n",
      " '2000-05-05' '2000-05-06' '2000-05-07' '2000-05-08' '2000-05-09'\n",
      " '2000-05-10' '2000-05-11' '2000-05-12' '2000-05-13' '2000-05-14'\n",
      " '2000-05-15' '2000-05-16' '2000-05-17' '2000-05-18' '2000-05-19'\n",
      " '2000-05-20' '2000-05-21' '2000-05-22' '2000-05-23' '2000-05-24'\n",
      " '2000-05-25' '2000-05-26' '2000-05-27' '2000-05-28' '2000-05-29'\n",
      " '2000-05-30' '2000-05-31' '2000-06-01' '2000-06-02' '2000-06-03'\n",
      " '2000-06-04' '2000-06-05' '2000-06-06' '2000-06-07' '2000-06-08'\n",
      " '2000-06-09' '2000-06-10' '2000-06-11' '2000-06-12' '2000-06-13'\n",
      " '2000-06-14' '2000-06-15' '2000-06-16' '2000-06-17' '2000-06-18'\n",
      " '2000-06-19' '2000-06-20' '2000-06-21' '2000-06-22' '2000-06-23'\n",
      " '2000-06-24' '2000-06-25' '2000-06-26' '2000-06-27' '2000-06-28'\n",
      " '2000-06-29' '2000-06-30' '2000-07-01' '2000-07-02' '2000-07-03'\n",
      " '2000-07-04' '2000-07-05' '2000-07-06' '2000-07-07' '2000-07-08'\n",
      " '2000-07-09' '2000-07-10' '2000-07-11' '2000-07-12' '2000-07-13'\n",
      " '2000-07-14' '2000-07-15' '2000-07-16' '2000-07-17' '2000-07-18'\n",
      " '2000-07-19' '2000-07-20' '2000-07-21' '2000-07-22' '2000-07-23'\n",
      " '2000-07-24' '2000-07-25' '2000-07-26' '2000-07-27' '2000-07-28'\n",
      " '2000-07-29' '2000-07-30' '2000-07-31' '2000-08-01' '2000-08-02'\n",
      " '2000-08-03' '2000-08-04' '2000-08-05' '2000-08-06' '2000-08-07'\n",
      " '2000-08-08' '2000-08-09' '2000-08-10' '2000-08-11' '2000-08-12'\n",
      " '2000-08-13' '2000-08-14' '2000-08-15' '2000-08-16' '2000-08-17'\n",
      " '2000-08-18' '2000-08-19' '2000-08-20' '2000-08-21' '2000-08-22'\n",
      " '2000-08-23' '2000-08-24' '2000-08-25' '2000-08-26' '2000-08-27'\n",
      " '2000-08-28' '2000-08-29' '2000-08-30' '2000-08-31' '2000-09-01'\n",
      " '2000-09-02' '2000-09-03' '2000-09-04' '2000-09-05' '2000-09-06'\n",
      " '2000-09-07' '2000-09-08' '2000-09-09' '2000-09-10' '2000-09-11'\n",
      " '2000-09-12' '2000-09-13' '2000-09-14' '2000-09-15' '2000-09-16'\n",
      " '2000-09-17' '2000-09-18' '2000-09-19' '2000-09-20' '2000-09-21'\n",
      " '2000-09-22' '2000-09-23' '2000-09-24' '2000-09-25' '2000-09-26'\n",
      " '2000-09-27' '2000-09-28' '2000-09-29' '2000-09-30' '2000-10-01'\n",
      " '2000-10-02' '2000-10-03' '2000-10-04' '2000-10-05' '2000-10-06'\n",
      " '2000-10-07' '2000-10-08' '2000-10-09' '2000-10-10' '2000-10-11'\n",
      " '2000-10-12' '2000-10-13' '2000-10-14' '2000-10-15' '2000-10-16'\n",
      " '2000-10-17' '2000-10-18' '2000-10-19' '2000-10-20' '2000-10-21'\n",
      " '2000-10-22' '2000-10-23' '2000-10-24' '2000-10-25' '2000-10-26'\n",
      " '2000-10-27' '2000-10-28' '2000-10-29' '2000-10-30' '2000-10-31'\n",
      " '2000-11-01' '2000-11-02' '2000-11-03' '2000-11-04' '2000-11-05'\n",
      " '2000-11-06' '2000-11-07' '2000-11-08' '2000-11-09' '2000-11-10'\n",
      " '2000-11-11' '2000-11-12' '2000-11-13' '2000-11-14' '2000-11-15'\n",
      " '2000-11-16' '2000-11-17' '2000-11-18' '2000-11-19' '2000-11-20'\n",
      " '2000-11-21' '2000-11-22' '2000-11-23' '2000-11-24' '2000-11-25'\n",
      " '2000-11-26' '2000-11-27' '2000-11-28' '2000-11-29' '2000-11-30'\n",
      " '2000-12-01' '2000-12-02' '2000-12-03' '2000-12-04' '2000-12-05'\n",
      " '2000-12-06' '2000-12-07' '2000-12-08' '2000-12-09' '2000-12-10'\n",
      " '2000-12-11' '2000-12-12' '2000-12-13' '2000-12-14' '2000-12-15'\n",
      " '2000-12-16' '2000-12-17' '2000-12-18' '2000-12-19' '2000-12-20'\n",
      " '2000-12-21' '2000-12-22' '2000-12-23' '2000-12-24' '2000-12-25'\n",
      " '2000-12-26' '2000-12-27' '2000-12-28' '2000-12-29' '2000-12-30'\n",
      " '2000-12-31' '2001-01-01' '2001-01-02' '2001-01-03' '2001-01-04'\n",
      " '2001-01-05' '2001-01-06' '2001-01-07' '2001-01-08' '2001-01-09'\n",
      " '2001-01-10' '2001-01-11' '2001-01-12' '2001-01-13' '2001-01-14'\n",
      " '2001-01-15' '2001-01-16' '2001-01-17' '2001-01-18' '2001-01-19'\n",
      " '2001-01-20' '2001-01-21' '2001-01-22' '2001-01-23' '2001-01-24'\n",
      " '2001-01-25' '2001-01-26' '2001-01-27' '2001-01-28' '2001-01-29'\n",
      " '2001-01-30' '2001-01-31' '2001-02-01' '2001-02-02' '2001-02-03'\n",
      " '2001-02-04' '2001-02-05' '2001-02-06' '2001-02-07' '2001-02-08'\n",
      " '2001-02-09' '2001-02-10' '2001-02-11' '2001-02-12' '2001-02-13'\n",
      " '2001-02-14' '2001-02-15' '2001-02-16' '2001-02-17' '2001-02-18'\n",
      " '2001-02-19' '2001-02-20' '2001-02-21' '2001-02-22' '2001-02-23'\n",
      " '2001-02-24' '2001-02-25' '2001-02-26' '2001-02-27' '2001-02-28'\n",
      " '2001-03-01' '2001-03-02' '2001-03-03' '2001-03-04' '2001-03-05'\n",
      " '2001-03-06' '2001-03-07' '2001-03-08' '2001-03-09' '2001-03-10'\n",
      " '2001-03-11' '2001-03-12' '2001-03-13' '2001-03-14' '2001-03-15'\n",
      " '2001-03-16' '2001-03-17' '2001-03-18' '2001-03-19' '2001-03-20'\n",
      " '2001-03-21' '2001-03-22' '2001-03-23' '2001-03-24' '2001-03-25'\n",
      " '2001-03-26' '2001-03-27' '2001-03-28' '2001-03-29' '2001-03-30'\n",
      " '2001-03-31' '2001-04-01' '2001-04-02' '2001-04-03' '2001-04-04'\n",
      " '2001-04-05' '2001-04-06' '2001-04-07' '2001-04-08' '2001-04-09'\n",
      " '2001-04-10' '2001-04-11' '2001-04-12' '2001-04-13' '2001-04-14'\n",
      " '2001-04-15' '2001-04-16' '2001-04-17' '2001-04-18' '2001-04-19'\n",
      " '2001-04-20' '2001-04-21' '2001-04-22' '2001-04-23' '2001-04-24'\n",
      " '2001-04-25' '2001-04-26' '2001-04-27' '2001-04-28' '2001-04-29'\n",
      " '2001-04-30' '2001-05-01' '2001-05-02' '2001-05-03' '2001-05-04'\n",
      " '2001-05-05' '2001-05-06' '2001-05-07' '2001-05-08' '2001-05-09'\n",
      " '2001-05-10' '2001-05-11' '2001-05-12' '2001-05-13' '2001-05-14'\n",
      " '2001-05-15' '2001-05-16' '2001-05-17' '2001-05-18' '2001-05-19'\n",
      " '2001-05-20' '2001-05-21' '2001-05-22' '2001-05-23' '2001-05-24'\n",
      " '2001-05-25' '2001-05-26' '2001-05-27' '2001-05-28' '2001-05-29'\n",
      " '2001-05-30' '2001-05-31' '2001-06-01' '2001-06-02' '2001-06-03'\n",
      " '2001-06-04' '2001-06-05' '2001-06-06' '2001-06-07' '2001-06-08'\n",
      " '2001-06-09' '2001-06-10' '2001-06-11' '2001-06-12' '2001-06-13'\n",
      " '2001-06-14' '2001-06-15' '2001-06-16' '2001-06-17' '2001-06-18'\n",
      " '2001-06-19' '2001-06-20' '2001-06-21' '2001-06-22' '2001-06-23'\n",
      " '2001-06-24' '2001-06-25' '2001-06-26' '2001-06-27' '2001-06-28'\n",
      " '2001-06-29' '2001-06-30' '2001-07-01' '2001-07-02' '2001-07-03'\n",
      " '2001-07-04' '2001-07-05' '2001-07-06' '2001-07-07' '2001-07-08'\n",
      " '2001-07-09' '2001-07-10' '2001-07-11' '2001-07-12' '2001-07-13'\n",
      " '2001-07-14' '2001-07-15' '2001-07-16' '2001-07-17' '2001-07-18'\n",
      " '2001-07-19' '2001-07-20' '2001-07-21' '2001-07-22' '2001-07-23'\n",
      " '2001-07-24' '2001-07-25' '2001-07-26' '2001-07-27' '2001-07-28'\n",
      " '2001-07-29' '2001-07-30' '2001-07-31' '2001-08-01' '2001-08-02'\n",
      " '2001-08-03' '2001-08-04' '2001-08-05' '2001-08-06' '2001-08-07'\n",
      " '2001-08-08' '2001-08-09' '2001-08-10' '2001-08-11' '2001-08-12'\n",
      " '2001-08-13' '2001-08-14' '2001-08-15' '2001-08-16' '2001-08-17'\n",
      " '2001-08-18' '2001-08-19' '2001-08-20' '2001-08-21' '2001-08-22'\n",
      " '2001-08-23' '2001-08-24' '2001-08-25' '2001-08-26' '2001-08-27'\n",
      " '2001-08-28' '2001-08-29' '2001-08-30' '2001-08-31' '2001-09-01'\n",
      " '2001-09-02' '2001-09-03' '2001-09-04' '2001-09-05' '2001-09-06'\n",
      " '2001-09-07' '2001-09-08' '2001-09-09' '2001-09-10' '2001-09-11'\n",
      " '2001-09-12' '2001-09-13' '2001-09-14' '2001-09-15' '2001-09-16'\n",
      " '2001-09-17' '2001-09-18' '2001-09-19' '2001-09-20' '2001-09-21'\n",
      " '2001-09-22' '2001-09-23' '2001-09-24' '2001-09-25' '2001-09-26'\n",
      " '2001-09-27' '2001-09-28' '2001-09-29' '2001-09-30' '2001-10-01'\n",
      " '2001-10-02' '2001-10-03' '2001-10-04' '2001-10-05' '2001-10-06'\n",
      " '2001-10-07' '2001-10-08' '2001-10-09' '2001-10-10' '2001-10-11'\n",
      " '2001-10-12' '2001-10-13' '2001-10-14']\n",
      "stop_time\n",
      "['00:01' '00:02' '00:05' ... '05:26' '04:17' '04:03']\n",
      "location_raw\n",
      "[nan 'F4' 'C3' 'E5' 'H6' 'E4' 'H1' 'A4' 'G5' 'D1' 'A3' 'H5' 'E1' 'G3' 'F1'\n",
      " 'B6' 'B5' 'A8' 'G1' 'G4' 'C1' 'A7' 'D5' 'H2' 'H3' 'C7' 'E6' 'C5' 'B8'\n",
      " 'E3' 'F5' 'E2' 'Stanly' 'C4' 'C6' 'A2' 'A6' 'F3' 'G2' 'B1' 'B7' 'C2' 'D3'\n",
      " 'A1' 'D2' 'A5' 'A9' 'G7' 'F2' 'D4' 'B3' 'B2' 'H4' 'D6' 'B4' 'G6'\n",
      " 'Montgomery' 'Bladen' 'Forsyth' 'Guilford' 'Caswell']\n",
      "county_name\n",
      "[nan 'Wake County' 'Forsyth County' 'Gaston County' 'Mecklenburg County'\n",
      " 'Davidson County' 'Burke County' 'Carteret County' 'Buncombe County'\n",
      " 'Alamance County' 'Cabarrus County' 'Harnett County' 'Rowan County'\n",
      " 'Stanly County' 'Johnston County' 'Cumberland County' 'Robeson County'\n",
      " 'Wayne County' 'Rockingham County' 'Guilford County' 'Onslow County'\n",
      " 'Sampson County' 'Cleveland County' 'Randolph County' 'Montgomery County'\n",
      " 'Bladen County' 'Caswell County']\n",
      "county_fips\n",
      "[   nan 37183. 37067. 37071. 37119. 37057. 37023. 37031. 37021. 37001.\n",
      " 37025. 37085. 37159. 37167. 37101. 37051. 37155. 37191. 37157. 37081.\n",
      " 37133. 37163. 37045. 37151. 37123. 37017. 37033.]\n",
      "fine_grained_location\n",
      "['Unknown' nan 'RALEIGH' 'DURHAM' 'HILLSBOROUGH' 'raleigh' 'WASHINGTON'\n",
      " 'SMITHFIELD' 'CHARLOTTE' 'CHAROLOTTE' 'goldsboro' 'WILSON' 'bryson'\n",
      " 'gastonia' 'Raleigh' '`' 'GREENVILLE' 'GASTONIA' 'MARION' 'durham'\n",
      " 'WASINGTON' 'greenville' 'ALBEMARLE' 'MORGANTON' 'gastonai'\n",
      " 'ROCKY MOOUNT' 'CONCORD' 'SHELBY' 'WAKE' 'Hillsboro' 'WASHIINGTON'\n",
      " 'GREENSBORO' 'greensboro' 'GASTONNA' 'SALISBURY' 'kenansville' 'q'\n",
      " 'ZEBULON' 'RALEGH' 'STATESVILLE' 'GRAHAM' 'Shelby' 'Bryson City'\n",
      " 'clayton' 'salisbury' 'HICKORY' 'graham' 'SSHELBY' 'SHELLBY' 'MONROE'\n",
      " 'SILER CITY' 'CLAYTON' 'cary' 'Asheville' 'holly springs'\n",
      " 'roanoke rapids' 'burnsville' 'washington' 'wake forest' 'monroe'\n",
      " 'Greenville' 'MOREHEAD' 'newport' 'charlotte' 'pittsboro' 'WAYNE'\n",
      " 'FAYETTEVILLE' 'Ayden' 'ayden' 'trenron' 'vanceboro' 'windsor' 'Cary'\n",
      " 'Charlotte' 'Statesville' 'Lexington' 'Greensboro' 'Hillsborough'\n",
      " 'LEXINGTON' 'statesville' 'Q' 'ahoskie' 'scotland neck' 'Salisbury'\n",
      " 'ASHEVILLE' 'PRINCETON' 'NEW BERN' 'BURLINGTON' 'WILKESBORO' 'JEFFERSON'\n",
      " 'GARNER' 'Kinston' 'Fuquay-Varina' 'N' 'garner' 'Garner' 'Albertson'\n",
      " 'BELHAVEN' 'wilmington' 'wilmingron' 'Wake Forest' 'lumberton'\n",
      " 'Pittsboro' 'FUQUAY VARINA' 'reidsville' 'WLLIAMSTON' 'WILLIAMSTON'\n",
      " 'RALEIG' 'Burlington' 'grimesland' 'D' 'CLINTON' 'GASTON' 'Benson'\n",
      " 'RALEIHG' 'RALEIGh' 'morganton' 'Goldsboro' 'LILLINGTON' 'KINSTON'\n",
      " 'lillington' 'Morganton' 'Ralleigh' 'FUQUAY-VARINA' 'MRGANTON' 'STOVALL'\n",
      " 'OXFORD' 'WAKE FOREST' 'ROCKY MOUNT' 'lexington' 'F' 'fayetteville'\n",
      " 'youngsville' 'Raleign' 'Sanford' 'Raleighf' 'CHATHAM' 'Fuquay Varina'\n",
      " 'New Bern' 'MORRISVILLE' 'Windsor' 'Edenton' 'WINSTON SALEM'\n",
      " 'CHAPEL HILL' 'Chapel Hill' 'CARY' 'smithfield' 'Wilson' 'Smithfield'\n",
      " 'Louisburg' '1' 'Winston Salem' 'JACKSONVILLE' 'colerain' 'edenton'\n",
      " 'beaufort' 'Jacksonville' 'Durham' 'GRANDY' 'HAVELOCK' 'clinton'\n",
      " 'WINDSOR' 'Lillington' 'Fayetteville' 'henderson' 'Roanoke Rapids'\n",
      " 'rALEIGH' 'JAMESVILLE' 'keenansville' 'MANTEO' 'Henderson'\n",
      " 'BURLINGTON ON I-85' 'BURLNGTON ON I-85' 'HILLSBOROUGH ON I-40'\n",
      " 'raleigh @ exit 11' 'Norlina' 'Raleigh on Hammonds' 'burgaw' 'Raleigh\\\\'\n",
      " 'Raleigh on I-40' 'asheville' 'Raleigh/I-440' 'Durham on nc147'\n",
      " 'Raleigh on I-440' 'CHAR' 'Raleigh/Brentwood' 'Raleigh/I-40'\n",
      " 'franklinton' 'Raleigh/Atlantic Ave' 'Raleigh/Hammonds Rd.' 'kinston'\n",
      " 'Raleigh/US1' 'TROY' 'PITTSBORO' 'High Point/I-85' 'Pollocksville'\n",
      " 'Concord' 'RALEIGH/I-440' 'AHOSKIE' 'RALEIGH/I-40' 'RALEIGH/US1'\n",
      " 'Alamance' 'burlington' 'TARBORO' 'hickory' 'FALKLAND' 'PACTOLUS'\n",
      " 'Clayton' 'LaGrange' '2' 'ROBERSONVILLE' 'Mocksville' 'VANCEBORO'\n",
      " 'GRIFTON' 'APEX' 'mocksvile' 'mocksville' 'asheboro' 'Havelock' 'murphy'\n",
      " 'ROANOKE RAPIDS' 'ROAN0KE RAPIDS' 'beaulaville' 'BEULAVILLE'\n",
      " 'Wilsonville' 'MOCKSVILLE' 'winston salem' 'Apex' 'SCOTLAND NECK'\n",
      " 'NEWPORT' 'GRANTSBORO' 'ELIZABETH CITY' 'salsibury' 'RICHMOND'\n",
      " 'FAYETTEVILE' 'KING' '~' 'king' 'Elizabeth City' 'Newton grove' 'Clinton'\n",
      " 'HENDERSON' 'Davidson' 'Rolesville' 'GRIMESLAND' 'Newton' 'Forsyth'\n",
      " 'CAPE CARTERET' 'WINTERVILLE' 'Yadkin' 'Conover' 'Ral' 'KNIGHTDALE'\n",
      " 'BENSON' 'Hickory' '10363' 'chapel hill' 'grifton' 'COLUMBIA']\n",
      "police_department\n",
      "['NC State Highway Patrol']\n",
      "driver_gender\n",
      "['M' 'F' nan]\n",
      "driver_age_raw\n",
      "[ 35.  20.  26.  48.  18.  25.  30.  40.  28.  21.  22.  31.  23.  27.\n",
      "  50.  44.  17.  19.  60.  33.  45.  37.  49.  24.  54.  34.  41.  63.\n",
      "  29.  67.  32.  16.  57.  36.  46.  86.  43.  42.  77.  59.  38.  51.\n",
      "  66.  65.  79.  39.  52.  55.  64.  58.  53.  85.  69.  47.  72.  71.\n",
      "  56.  15.  74.  62.  61.  68.  76.  12.  81.  70.  73.  78.  75.  84.\n",
      "  11.  83.  80.  14.  92.  90.  82.  97.  13.  87.  10.  91.  89.  94.\n",
      "  93.  96.  88.  95.  99.  98.   9. 101. 224.  nan 243. 255.   3. 233.]\n",
      "driver_age\n",
      "[35. 20. 26. 48. 18. 25. 30. 40. 28. 21. 22. 31. 23. 27. 50. 44. 17. 19.\n",
      " 60. 33. 45. 37. 49. 24. 54. 34. 41. 63. 29. 67. 32. 16. 57. 36. 46. 86.\n",
      " 43. 42. 77. 59. 38. 51. 66. 65. 79. 39. 52. 55. 64. 58. 53. 85. 69. 47.\n",
      " 72. 71. 56. 15. 74. 62. 61. 68. 76. nan 81. 70. 73. 78. 75. 84. 83. 80.\n",
      " 92. 90. 82. 97. 87. 91. 89. 94. 93. 96. 88. 95. 99. 98.]\n",
      "driver_race_raw\n",
      "['B N' 'W N' 'U N' 'U H' 'I N' 'W H' 'A N' 'A H' 'B H' 'I H' 'NA NA']\n",
      "driver_race\n",
      "['Black' 'White' 'Other' 'Hispanic' 'Asian' nan]\n",
      "violation_raw\n",
      "['Vehicle Equipment Violation' 'Other Motor Vehicle Violation'\n",
      " 'Safe Movement Violation' 'Speed Limit Violation' 'Impaired Driving'\n",
      " 'Seat Belt Violation' 'Investigation' 'Traffic Light/Sign Violation'\n",
      " 'Vehicle Regulatory Violation']\n",
      "violation\n",
      "['Equipment' 'Other' 'Safe movement' 'Speeding' 'DUI' 'Seat belt'\n",
      " 'Stop sign/light' 'Registration/plates']\n",
      "search_conducted\n",
      "[ True False]\n",
      "search_type_raw\n",
      "['Search Incident to Arrest' nan 'Consent' 'Protective Frisk'\n",
      " 'Search Warrant' 'Probable Cause']\n",
      "search_type\n",
      "['Incident to Arrest' nan 'Consent' 'Protective Frisk' 'Warrant'\n",
      " 'Probable Cause']\n",
      "contraband_found\n",
      "[False  True]\n",
      "stop_outcome\n",
      "['Arrest' 'Written Warning' 'Citation' 'No Action' 'Verbal Warning']\n",
      "is_arrested\n",
      "[ True False]\n",
      "search_basis\n",
      "[nan 'Observation Suspected Contraband' 'Witness Observation'\n",
      " 'Suspicious Movement' 'Other Official Info'\n",
      " 'Erratic Suspicious Behaviour'\n",
      " 'Observation Suspected Contraband,Informant Tip' 'Informant Tip'\n",
      " 'Erratic Suspicious Behaviour,Observation Suspected Contraband'\n",
      " 'Observation Suspected Contraband,Suspicious Movement'\n",
      " 'Erratic Suspicious Behaviour,Witness Observation'\n",
      " 'Erratic Suspicious Behaviour,Suspicious Movement'\n",
      " 'Other Official Info,Suspicious Movement,Witness Observation'\n",
      " 'Other Official Info,Witness Observation'\n",
      " 'Erratic Suspicious Behaviour,Observation Suspected Contraband,Suspicious Movement'\n",
      " 'Suspicious Movement,Witness Observation'\n",
      " 'Observation Suspected Contraband,Other Official Info'\n",
      " 'Erratic Suspicious Behaviour,Other Official Info'\n",
      " 'Observation Suspected Contraband,Witness Observation'\n",
      " 'Erratic Suspicious Behaviour,Observation Suspected Contraband,Other Official Info'\n",
      " 'Erratic Suspicious Behaviour,Observation Suspected Contraband,Other Official Info,Suspicious Movement'\n",
      " 'Observation Suspected Contraband,Other Official Info,Suspicious Movement,Informant Tip,Witness Observation'\n",
      " 'Erratic Suspicious Behaviour,Other Official Info,Informant Tip,Witness Observation'\n",
      " 'Erratic Suspicious Behaviour,Informant Tip'\n",
      " 'Other Official Info,Suspicious Movement'\n",
      " 'Erratic Suspicious Behaviour,Other Official Info,Suspicious Movement']\n",
      "officer_id\n",
      "[nan '11162' '11370' ... '1085' '0818' 'n10238']\n",
      "drugs_related_stop\n",
      "[nan True]\n",
      "ethnicity\n",
      "['N' 'H' nan]\n",
      "district\n",
      "[nan 'F4' 'C3' 'E5' 'H6' 'E4' 'H1' 'A4' 'G5' 'D1' 'A3' 'H5' 'E1' 'G3' 'F1'\n",
      " 'B6' 'B5' 'A8' 'G1' 'G4' 'C1' 'A7' 'D5' 'H2' 'H3' 'C7' 'E6' 'C5' 'B8'\n",
      " 'E3' 'F5' 'E2' 'C4' 'C6' 'A2' 'A6' 'F3' 'G2' 'B1' 'B7' 'C2' 'D3' 'A1'\n",
      " 'D2' 'A5' 'A9' 'G7' 'F2' 'D4' 'B3' 'B2' 'H4' 'D6' 'B4' 'G6']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Loading and Observing Data first 1 million rows\n",
    "tf_df = pd.read_csv(\"NC_cleaned.csv\", low_memory = False, nrows= 1_000_000)\n",
    "tf_df.head()\n",
    "\n",
    "#Checking for unique values for each feature (viewed in seperate text editor)\n",
    "for col in tf_df.columns:\n",
    "    print(col)\n",
    "    print(tf_df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 27 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   id                     1000000 non-null  object \n",
      " 1   state                  1000000 non-null  object \n",
      " 2   stop_date              1000000 non-null  object \n",
      " 3   stop_time              335025 non-null   object \n",
      " 4   location_raw           667185 non-null   object \n",
      " 5   county_name            289288 non-null   object \n",
      " 6   county_fips            289288 non-null   float64\n",
      " 7   fine_grained_location  338426 non-null   object \n",
      " 8   police_department      1000000 non-null  object \n",
      " 9   driver_gender          999997 non-null   object \n",
      " 10  driver_age_raw         999997 non-null   float64\n",
      " 11  driver_age             999802 non-null   float64\n",
      " 12  driver_race_raw        1000000 non-null  object \n",
      " 13  driver_race            999997 non-null   object \n",
      " 14  violation_raw          1000000 non-null  object \n",
      " 15  violation              1000000 non-null  object \n",
      " 16  search_conducted       1000000 non-null  bool   \n",
      " 17  search_type_raw        12781 non-null    object \n",
      " 18  search_type            12781 non-null    object \n",
      " 19  contraband_found       1000000 non-null  bool   \n",
      " 20  stop_outcome           1000000 non-null  object \n",
      " 21  is_arrested            1000000 non-null  bool   \n",
      " 22  search_basis           12694 non-null    object \n",
      " 23  officer_id             667185 non-null   object \n",
      " 24  drugs_related_stop     570 non-null      object \n",
      " 25  ethnicity              999997 non-null   object \n",
      " 26  district               667185 non-null   object \n",
      "dtypes: bool(3), float64(3), object(21)\n",
      "memory usage: 186.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#Getting More dataset information noteably for nulls\n",
    "tf_df.info()\n",
    "\n",
    "#Convert is_arrested and search_conducted\n",
    "tf_df.replace({False: 0, True: 1}, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "Report the percentage of the Arrested per each category, where percentage is calculated based on the total number of samples from that category.\n",
    "\n",
    "> Race: Asian, Black, Hispanic, White\n",
    "\n",
    "> Age: 15-19, 20-29, 30-39, 40-49, 50+\n",
    "\n",
    "> Gender: female, male \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         counts  normalized_counts\n",
      "driver_race is_arrested                           \n",
      "Asian       0              6492           0.984382\n",
      "            1               103           0.015618\n",
      "Black       0            218454           0.968990\n",
      "            1              6991           0.031010\n",
      "Hispanic    0             56337           0.929592\n",
      "            1              4267           0.070408\n",
      "Other       0             22378           0.968367\n",
      "            1               731           0.031633\n",
      "White       0            671254           0.981016\n",
      "            1             12990           0.018984\n"
     ]
    }
   ],
   "source": [
    "#Race\n",
    "print(pd.concat(\n",
    "    [\n",
    "        #Number of samples (value_counts() does not count na values)\n",
    "        tf_df.groupby(\"driver_race\")[\"is_arrested\"].value_counts(),\n",
    "\n",
    "        #Percentage of samples\n",
    "        tf_df.groupby(\"driver_race\")[\"is_arrested\"].value_counts(normalize = True)\n",
    "    ],\n",
    "    keys=['counts', 'normalized_counts'],\n",
    "    axis=1,\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.0\n",
      "                        counts  normalized_counts\n",
      "driver_age is_arrested                           \n",
      "[15, 20)   0            111641           0.984315\n",
      "           1              1779           0.015685\n",
      "[20, 30)   0            358486           0.971638\n",
      "           1             10464           0.028362\n",
      "[30, 40)   0            237122           0.971851\n",
      "           1              6868           0.028149\n",
      "[40, 50)   0            149297           0.972936\n",
      "           1              4153           0.027064\n",
      "[50, 100)  0            118180           0.984899\n",
      "           1              1812           0.015101\n"
     ]
    }
   ],
   "source": [
    "#Age\n",
    "\n",
    "#Check max age for bins\n",
    "print(tf_df.driver_age.max())\n",
    "\n",
    "#Group by bins, include minimum value exclude top value. ( ]\n",
    "print(\n",
    "    pd.concat(\n",
    "        [\n",
    "tf_df.groupby(pd.cut(tf_df[\"driver_age\"],\n",
    "                    bins = [15,20,30,40,50,100], \n",
    "                    include_lowest= True, \n",
    "                    right = False))[\"is_arrested\"].value_counts(),\n",
    "\n",
    "tf_df.groupby(pd.cut(tf_df[\"driver_age\"],\n",
    "                    bins = [15,20,30,40,50,100], \n",
    "                    include_lowest= True, \n",
    "                    right = False))[\"is_arrested\"].value_counts(normalize = True)],\n",
    "                \n",
    "                 keys=['counts', 'normalized_counts'],\n",
    "                 axis=1,\n",
    "                    \n",
    "            ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           counts  normalized_counts\n",
      "driver_gender is_arrested                           \n",
      "F             0            315028           0.988841\n",
      "              1              3555           0.011159\n",
      "M             0            659887           0.968408\n",
      "              1             21527           0.031592\n"
     ]
    }
   ],
   "source": [
    "#Gender\n",
    "print(pd.concat(\n",
    "    [\n",
    "        #Number of samples \n",
    "        tf_df.groupby(\"driver_gender\")[\"is_arrested\"].value_counts(),\n",
    "\n",
    "        #Percentage of samples\n",
    "        tf_df.groupby(\"driver_gender\")[\"is_arrested\"].value_counts(normalize = True)\n",
    "    ],\n",
    "    keys=['counts', 'normalized_counts'],\n",
    "    axis=1,\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Is the above metric a good indication of the biases that could be seen in the dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No they aren't. This is because the distribution of the samples are widely uneven between the classes. Also the effects of different variables are different such as all 3 of the features we just analyzed (race, age and gender). In race for example, the first million records I sampled contains 6492 falling in the not arrested catagory. This is compared to the 671254 samples the class \"White\" has which is vastly different and affects representation and generalization considerations in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "Controlling for age (bucketed as in part 1), gender, year use logistic regression to estimate impact of race on: search_conducted, is_arrested and stop_outcome == \"Citation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing Feature Method\n",
    "def data_transform(df):\n",
    "    binary_data = pd.get_dummies(df)\n",
    "    feature_cols = binary_data[binary_data.columns[:]]\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(feature_cols), columns=feature_cols.columns)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 142420 entries, 1 to 564226\n",
      "Data columns (total 27 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   id                     142420 non-null  object \n",
      " 1   state                  142420 non-null  object \n",
      " 2   stop_date              142420 non-null  object \n",
      " 3   stop_time              84431 non-null   object \n",
      " 4   location_raw           58251 non-null   object \n",
      " 5   county_name            26630 non-null   object \n",
      " 6   county_fips            26630 non-null   float64\n",
      " 7   fine_grained_location  85235 non-null   object \n",
      " 8   police_department      142420 non-null  object \n",
      " 9   driver_gender          142420 non-null  object \n",
      " 10  driver_age_raw         142420 non-null  float64\n",
      " 11  driver_age             142420 non-null  float64\n",
      " 12  driver_race_raw        142420 non-null  object \n",
      " 13  driver_race            142420 non-null  object \n",
      " 14  violation_raw          142420 non-null  object \n",
      " 15  violation              142420 non-null  object \n",
      " 16  search_conducted       142420 non-null  int64  \n",
      " 17  search_type_raw        2546 non-null    object \n",
      " 18  search_type            2546 non-null    object \n",
      " 19  contraband_found       142420 non-null  int64  \n",
      " 20  stop_outcome           142420 non-null  object \n",
      " 21  is_arrested            142420 non-null  int64  \n",
      " 22  search_basis           2514 non-null    object \n",
      " 23  officer_id             58251 non-null   object \n",
      " 24  drugs_related_stop     84 non-null      float64\n",
      " 25  ethnicity              142420 non-null  object \n",
      " 26  district               58251 non-null   object \n",
      "dtypes: float64(4), int64(3), object(20)\n",
      "memory usage: 30.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Other</th>\n",
       "      <th>White</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>-0.524665</td>\n",
       "      <td>-0.355931</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>-0.524665</td>\n",
       "      <td>-0.355931</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>1.905977</td>\n",
       "      <td>-0.355931</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>-1.310959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>-0.524665</td>\n",
       "      <td>-0.355931</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>1.905977</td>\n",
       "      <td>-0.355931</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>-1.310959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142415</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>-0.524665</td>\n",
       "      <td>2.809532</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>-1.310959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142416</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>-0.524665</td>\n",
       "      <td>-0.355931</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142417</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>-0.524665</td>\n",
       "      <td>-0.355931</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142418</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>1.905977</td>\n",
       "      <td>-0.355931</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>-1.310959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142419</th>\n",
       "      <td>-0.094288</td>\n",
       "      <td>-0.524665</td>\n",
       "      <td>-0.355931</td>\n",
       "      <td>-0.178045</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142420 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Asian     Black  Hispanic     Other     White\n",
       "0      -0.094288 -0.524665 -0.355931 -0.178045  0.762800\n",
       "1      -0.094288 -0.524665 -0.355931 -0.178045  0.762800\n",
       "2      -0.094288  1.905977 -0.355931 -0.178045 -1.310959\n",
       "3      -0.094288 -0.524665 -0.355931 -0.178045  0.762800\n",
       "4      -0.094288  1.905977 -0.355931 -0.178045 -1.310959\n",
       "...          ...       ...       ...       ...       ...\n",
       "142415 -0.094288 -0.524665  2.809532 -0.178045 -1.310959\n",
       "142416 -0.094288 -0.524665 -0.355931 -0.178045  0.762800\n",
       "142417 -0.094288 -0.524665 -0.355931 -0.178045  0.762800\n",
       "142418 -0.094288  1.905977 -0.355931 -0.178045 -1.310959\n",
       "142419 -0.094288 -0.524665 -0.355931 -0.178045  0.762800\n",
       "\n",
       "[142420 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Subsetting dataframe Male, 20-29 Age, Year=2010 (Control Variables)\n",
    "data_controlled = tf_df[(tf_df['driver_gender'] == \"M\") & (tf_df[\"stop_date\"].str.contains(\"2000\")) & (tf_df[\"driver_age\"] >= 20) & (tf_df[\"driver_age\"] <=29)]\n",
    "data_controlled.info()\n",
    "#Creating dataframe with race (target feature) to normalize \n",
    "data_controlled_race = data_controlled['driver_race']\n",
    "\n",
    "#Using Normalizing method\n",
    "data = data_transform(data_controlled_race)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Conducted Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Conducted Probabilities: \n",
      "[[0.98800193 0.01199807]\n",
      " [0.98800193 0.01199807]\n",
      " [0.97596989 0.02403011]\n",
      " ...\n",
      " [0.98800193 0.01199807]\n",
      " [0.97596989 0.02403011]\n",
      " [0.98800193 0.01199807]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "X = data\n",
    "y = data_controlled.search_conducted\n",
    "\n",
    "# Split X and y into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)\n",
    "\n",
    "# Instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "# Fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Find Probabilities\n",
    "y_prob = logreg.predict_proba(X)\n",
    "print (\"Search Conducted Probabilities: \\n\" + str(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Condcuted Coefficients (Asian, Black, Hispanic, Other, White) : \n",
      "[[-0.12107505  0.10169805  0.24460957 -0.00534149 -0.22163781]]\n"
     ]
    }
   ],
   "source": [
    "#Find Coefficients\n",
    "w= logreg.coef_\n",
    "print(\"Search Condcuted Coefficients (Asian, Black, Hispanic, Other, White) : \\n\" + str(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Conducted Standard Error is :0.017900107662781446\n"
     ]
    }
   ],
   "source": [
    "#Standard Error Report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res=logreg.predict(X_train)\n",
    "print(\"Search Conducted Standard Error is :\" +  str(mean_squared_error(y_train,res)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrested Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrested Probabilities: \n",
      "[[0.97731201 0.02268799]\n",
      " [0.97731201 0.02268799]\n",
      " [0.96192157 0.03807843]\n",
      " ...\n",
      " [0.97731201 0.02268799]\n",
      " [0.96192157 0.03807843]\n",
      " [0.97731201 0.02268799]]\n"
     ]
    }
   ],
   "source": [
    "X = data\n",
    "y = data_controlled.is_arrested\n",
    "\n",
    "# Split X and y into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)\n",
    "\n",
    "# Instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "# Fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Find Probabilities\n",
    "y_prob = logreg.predict_proba(X)\n",
    "print (\"Arrested Probabilities: \\n\" + str(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrested Coefficients (Asian, Black, Hispanic, Other, White) : [[-0.1473407   0.04224526  0.29229119  0.02476736 -0.20783627]]\n"
     ]
    }
   ],
   "source": [
    "#Find Coefficients\n",
    "w= logreg.coef_\n",
    "print(\"Arrested Coefficients (Asian, Black, Hispanic, Other, White) : \" + str(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrested Standard Error is :0.03316949866591771\n"
     ]
    }
   ],
   "source": [
    "res=logreg.predict(X_train)\n",
    "print(\"Arrested Standard Error is :\" +  str(mean_squared_error(y_train,res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Stop Outcome is a citation Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to create and normalize new dataset since there are fewer records with \"Citation\" I need to convert\n",
    "data_controlled_citation = data_controlled.copy()\n",
    "data_controlled_citation.replace({'Arrest': 0,'Written Warning': 0, 'No Action' : 0,'Verbal Warning' :0,  'Citation': 1}, inplace=True)\n",
    "data_citation_race = data_controlled_citation[\"driver_race\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Recieved Probabilities: \n",
      "[[0.19574498 0.80425502]\n",
      " [0.19574498 0.80425502]\n",
      " [0.21849071 0.78150929]\n",
      " ...\n",
      " [0.19574498 0.80425502]\n",
      " [0.21849071 0.78150929]\n",
      " [0.19574498 0.80425502]]\n"
     ]
    }
   ],
   "source": [
    "X = data\n",
    "y = data_controlled_citation.stop_outcome\n",
    "\n",
    "# Split X and y into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)\n",
    "\n",
    "# Instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "# Fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Probability of Citation\n",
    "y_prob = logreg.predict_proba(X)\n",
    "print (\"Citation Recieved Probabilities: \\n\" + str(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Coefficients (Asian, Black, Hispanic, Other, White) : [[ 0.02334619 -0.0303915  -0.02152667  0.01197036  0.03122316]]\n"
     ]
    }
   ],
   "source": [
    "# Citation Coefficients\n",
    "w= logreg.coef_\n",
    "print(\"Citation Coefficients (Asian, Black, Hispanic, Other, White) : \" + str(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Standard Error is :0.20285540420352946\n"
     ]
    }
   ],
   "source": [
    "#Standard Error\n",
    "res=logreg.predict(X_train)\n",
    "print(\"Citation Standard Error is :\" +  str(mean_squared_error(y_train,res)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C\n",
    "Interpret the coefficients you reported in part B."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is the ratio of the probability of search of Hispanic drivers to White drivers? Black drivers to White drivers?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hispanic-White : \n",
    "    The ratio of search was much higher in Hispanic versus White. The coefficient for the Hispanic class was .2446. For White, the b value was actually negative coming in at -.2216. \n",
    "    - Computing the white coefficient we can see that e^-.2216 = .801. 1 -.801 = .1988 which equates to about a 20% less relative chance to get searched at a traffic stop. \n",
    "    - Hispanics have an odd ratio of e^.2445 = 1.2771. If we subtract 1 we can see that hispanics are approximately 28% more likely to get searched at a traffic stop.\n",
    "\n",
    "Black - White :\n",
    "    -For Black with a much higher coefficient than white, e^.1017 = 1.1071. This equates to about a 11% higher chance to get searched."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Repeat the above for the probability of arrest instead of search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hispanic - White:\n",
    "-Hispanic has the higher coefficient. By calculating for odds ratio, e^.2923 = 1.3400 which is about a 34% higher chance to get arrested.\n",
    "-White coefficient is once again negative at -.2078. e^-.2078 = .8124. By subracting this from 1 we get approx. 19% less chance to get arrested at a stop\n",
    "\n",
    "Black - White:\n",
    "-Black coeff is much higher than all groups at .4225. Calculations e^.4425 = 1.557. This amounts to a high percentage of a 56% relative chance to get arrested."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What is the difference in citation probability between Hispanic drivers and White drivers? Black drivers and White drivers?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across the three groups the coefficients are as follows:\n",
    " Black : -.0303 \n",
    " Hispanic: -.0215\n",
    " White: .0312\n",
    "\n",
    " The probability of citations are extremely close among the three groups being that the coefficients are close to one.On all three. They range from -.03 in the black group and .0312 in the white group. Probabilities do indicate that White has a very slight increased probability to get a citation. Hispanic and Black have a slightly lower chance of getting one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    ">Explain in a sentence or two why we should control for variables such as gender and location in the regression, and why the results might not be what we want if we don’t control for them. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of controlling variables in logistical regression is to eliminate other influencing factors or alternative explanations. In this case we are specifically seeing the independent variables (race) effect on the targets after including controlled variables. Analysis and prediction becomes more complex if we have to account for gender and race for example Black Male versus White Female in an arrest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    ">However, decisions about what to control are somewhat subjective. What is one reason we might not want to control for location in testing for discrimination? In other words, how might we underestimate discrimination if we control for location? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we control for location we underestimate discrimination by possibly zoning in on an area with skewed distributions of race within a population of citizens and the cops as well. Crime operations can also be rooted in certain geographic location furthering discrimination in policing in this area. Officers may be accustomed to higher stake situations and be on increased alert no matter the race during a traffic stop. Discrimination can be underestimated on various scales when talking about global levels. Counries can widely differ in policing systems and protocols."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "460b781dad6108c2e2b927985b8df4a9aa8b01088baecdc4d14ba6677b90611d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
